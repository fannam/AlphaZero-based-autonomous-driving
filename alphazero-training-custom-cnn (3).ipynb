{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afb8d77",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-31T02:29:26.379258Z",
     "iopub.status.busy": "2024-12-31T02:29:26.378904Z",
     "iopub.status.idle": "2024-12-31T02:30:31.634330Z",
     "shell.execute_reply": "2024-12-31T02:30:31.633256Z"
    },
    "papermill": {
     "duration": 65.263874,
     "end_time": "2024-12-31T02:30:31.636114",
     "exception": false,
     "start_time": "2024-12-31T02:29:26.372240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting highway-env\r\n",
      "  Downloading highway_env-1.10.1-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.0)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\r\n",
      "Collecting gymnasium\r\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from highway-env) (0.0.4)\r\n",
      "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from highway-env) (2.6.0)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from highway-env) (3.7.1)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from highway-env) (2.1.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from highway-env) (1.13.1)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (24.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway-env) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway-env) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.16.0)\r\n",
      "Downloading highway_env-1.10.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: gymnasium, highway-env\r\n",
      "  Attempting uninstall: gymnasium\r\n",
      "    Found existing installation: gymnasium 0.29.0\r\n",
      "    Uninstalling gymnasium-0.29.0:\r\n",
      "      Successfully uninstalled gymnasium-0.29.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "kaggle-environments 1.16.10 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\r\n",
      "stable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed gymnasium-1.0.0 highway-env-1.10.1\r\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\r\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\r\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\r\n",
      "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\r\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\r\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\r\n",
      "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\r\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\r\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\r\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,197 kB]\r\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\r\n",
      "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\r\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\r\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,633 kB]\r\n",
      "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\r\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\r\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,840 kB]\r\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\r\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\r\n",
      "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,631 kB]\r\n",
      "Get:21 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\r\n",
      "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,563 kB]\r\n",
      "Get:23 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.4 kB]\r\n",
      "Fetched 28.2 MB in 2s (13.0 MB/s)\r\n",
      "\r\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "build-essential is already the newest version (12.9ubuntu3).\r\n",
      "Suggested packages:\r\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  swig swig4.0\r\n",
      "0 upgraded, 2 newly installed, 0 to remove and 136 not upgraded.\r\n",
      "Need to get 1,116 kB of archives.\r\n",
      "After this operation, 5,542 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\r\n",
      "Fetched 1,116 kB in 0s (4,298 kB/s)\r\n",
      "debconf: unable to initialize frontend: Dialog\r\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\r\n",
      "debconf: falling back to frontend: Readline\r\n",
      "Selecting previously unselected package swig4.0.\r\n",
      "(Reading database ... 127365 files and directories currently installed.)\r\n",
      "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\r\n",
      "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\r\n",
      "Selecting previously unselected package swig.\r\n",
      "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\r\n",
      "Unpacking swig (4.0.2-1ubuntu1) ...\r\n",
      "Setting up swig4.0 (4.0.2-1ubuntu1) ...\r\n",
      "Setting up swig (4.0.2-1ubuntu1) ...\r\n",
      "Processing triggers for man-db (2.10.2-1) ...\r\n",
      "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\r\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\r\n",
      "Collecting swig==4.* (from gymnasium[box2d])\r\n",
      "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\r\n",
      "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\r\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349114 sha256=2ee5ef75bc4889f6a1fafd656c754bdc8e1b978630ea5fd1c6366cedb2930a14\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\r\n",
      "Successfully built box2d-py\r\n",
      "Installing collected packages: swig, box2d-py\r\n",
      "Successfully installed box2d-py-2.3.5 swig-4.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install highway-env gymnasium torch numpy tqdm\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install build-essential swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3de6cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:31.658220Z",
     "iopub.status.busy": "2024-12-31T02:30:31.657876Z",
     "iopub.status.idle": "2024-12-31T02:30:34.644797Z",
     "shell.execute_reply": "2024-12-31T02:30:34.643773Z"
    },
    "papermill": {
     "duration": 2.999786,
     "end_time": "2024-12-31T02:30:34.646611",
     "exception": false,
     "start_time": "2024-12-31T02:30:31.646825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlphaZeroNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, n_residual_layers=10, n_actions=5):\n",
    "        super(AlphaZeroNetwork, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Convolution đầu tiên\n",
    "        self.conv_layer = nn.Conv2d(input_shape[2], 128, kernel_size=11, padding=5)\n",
    "        self.batch_norm = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Residual layers\n",
    "        self.residual_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 128, kernel_size=7, padding=3),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128)\n",
    "            )\n",
    "            for _ in range(n_residual_layers)\n",
    "        ])\n",
    "\n",
    "        # Value head\n",
    "        self.value_conv = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(1)\n",
    "        self.fc_input_size = input_shape[0] * input_shape[1]\n",
    "\n",
    "        self.value_fc1 = nn.Linear(self.fc_input_size, 128)\n",
    "        self.value_dropout = nn.Dropout(0.1)  # Thêm Dropout\n",
    "        self.value_fc2 = nn.Linear(128, 1)\n",
    "\n",
    "        # Policy head\n",
    "        self.policy_conv = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(2)\n",
    "        self.policy_fc = nn.Linear(self.fc_input_size * 2, n_actions)\n",
    "        self.policy_dropout = nn.Dropout(0.1)  # Thêm Dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution đầu tiên và batch normalization\n",
    "        x = F.relu(self.batch_norm(self.conv_layer(x)))\n",
    "\n",
    "        # Residual layers\n",
    "        for residual in self.residual_layers:\n",
    "            residual_x = x\n",
    "            x = residual(x) + residual_x\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Value head\n",
    "        value = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        value = value.reshape(value.size(0), -1)\n",
    "        value = F.relu(self.value_fc1(value))\n",
    "        value = self.value_dropout(value)  # Áp dụng Dropout\n",
    "        value = torch.tanh(self.value_fc2(value))\n",
    "\n",
    "        # Policy head\n",
    "        policy = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        policy = policy.reshape(policy.size(0), -1)\n",
    "        policy = self.policy_fc(self.policy_dropout(policy))  # Áp dụng Dropout\n",
    "        policy = F.softmax(policy, dim=1)\n",
    "\n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d83535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:34.668512Z",
     "iopub.status.busy": "2024-12-31T02:30:34.668059Z",
     "iopub.status.idle": "2024-12-31T02:30:34.683229Z",
     "shell.execute_reply": "2024-12-31T02:30:34.682521Z"
    },
    "papermill": {
     "duration": 0.027345,
     "end_time": "2024-12-31T02:30:34.684404",
     "exception": false,
     "start_time": "2024-12-31T02:30:34.657059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KinematicToGridWrapper:\n",
    "    def __init__(self):\n",
    "        # Define grid parameters\n",
    "        self.x_range = (-30, 90)  # meters relative to ego\n",
    "        self.y_range = (-10, 10)   # meters relative to ego\n",
    "\n",
    "        # Calculate grid size based on 1m per cell\n",
    "        self.grid_size = (\n",
    "            self.x_range[1] - self.x_range[0],  # 120 cells for x\n",
    "            self.y_range[1] - self.y_range[0]   # 20 cells for y\n",
    "        )\n",
    "\n",
    "        # Car dimensions\n",
    "        self.car_length = 5  # meters\n",
    "        self.car_width = 2   # meters\n",
    "\n",
    "    def get_car_footprint(self, x, y, heading):\n",
    "        \"\"\"Calculate which cells a car occupies given its center and heading\"\"\"\n",
    "        occupied_cells = []\n",
    "\n",
    "        cos_h = np.cos(heading)\n",
    "        sin_h = np.sin(heading)\n",
    "\n",
    "        corners_car = [\n",
    "            (-self.car_length/2, -self.car_width/2),\n",
    "            (self.car_length/2, -self.car_width/2),\n",
    "            (self.car_length/2, self.car_width/2),\n",
    "            (-self.car_length/2, self.car_width/2)\n",
    "        ]\n",
    "\n",
    "        corners_world = [\n",
    "            (x + dx*cos_h - dy*sin_h, y + dx*sin_h + dy*cos_h)\n",
    "            for dx, dy in corners_car\n",
    "        ]\n",
    "\n",
    "        min_x = min(x[0] for x in corners_world)\n",
    "        max_x = max(x[0] for x in corners_world)\n",
    "        min_y = min(x[1] for x in corners_world)\n",
    "        max_y = max(x[1] for x in corners_world)\n",
    "\n",
    "        for cell_x in range(int(min_x), int(max_x) + 1):\n",
    "            for cell_y in range(int(min_y), int(max_y) + 1):\n",
    "                if self.point_in_rotated_rect(\n",
    "                    cell_x + 0.5, cell_y + 0.5,\n",
    "                    x, y, heading,\n",
    "                    self.car_length, self.car_width\n",
    "                ):\n",
    "                    occupied_cells.append((cell_x, cell_y))\n",
    "\n",
    "        return occupied_cells\n",
    "\n",
    "    def world_to_grid(self, x, y):\n",
    "        \"\"\"Convert world coordinates to grid coordinates\"\"\"\n",
    "        grid_x = int(x - self.x_range[0])\n",
    "        # Flip y-axis to maintain correct orientation\n",
    "        grid_y = int(self.grid_size[1] - (y - self.y_range[0]) - 1)\n",
    "        return grid_x, grid_y\n",
    "\n",
    "    def point_in_rotated_rect(self, px, py, rect_x, rect_y, rect_angle, length, width):\n",
    "        dx = px - rect_x\n",
    "        dy = py - rect_y\n",
    "\n",
    "        cos_h = np.cos(-rect_angle)\n",
    "        sin_h = np.sin(-rect_angle)\n",
    "\n",
    "        rotated_x = dx * cos_h - dy * sin_h\n",
    "        rotated_y = dx * sin_h + dy * cos_h\n",
    "\n",
    "        return (abs(rotated_x) <= length/2) and (abs(rotated_y) <= width/2)\n",
    "\n",
    "    def process_observation(self, obs, left_bound, right_bound):\n",
    "        \"\"\"\n",
    "        Process vehicle observations and return separate ego info and occupancy grid\n",
    "        obs: list of [x, y, vx, vy, heading] for each vehicle (ego first)\n",
    "        \"\"\"\n",
    "        # Extract ego vehicle state\n",
    "        ego_x, ego_y, ego_vx, ego_vy, ego_heading = obs[0]\n",
    "\n",
    "        # Initialize grid\n",
    "        grid = np.zeros((self.grid_size[0], self.grid_size[1], 3), dtype=np.float32)\n",
    "\n",
    "        to_left =  ego_y - left_bound\n",
    "        to_right = right_bound - ego_y\n",
    "\n",
    "        left = int(self.grid_size[1]/2 -1 - to_left)\n",
    "        right = int(self.grid_size[1]/2 +1 + to_right)\n",
    "\n",
    "        if left >= 0:\n",
    "            grid[:, :left + 1, 0] = 2\n",
    "            grid[:, :left + 1, 2] = ego_vy\n",
    "        if right < self.grid_size[1]:\n",
    "            grid[:, right:, 0] = 2\n",
    "            grid[:, right:, 2] = ego_vy\n",
    "\n",
    "        # Place ego vehicle\n",
    "        ego_cells = self.get_car_footprint(0, 0, ego_heading)\n",
    "        for cell_x, cell_y in ego_cells:\n",
    "            grid_x, grid_y = self.world_to_grid(cell_x, cell_y)\n",
    "\n",
    "            if (0 <= grid_x < self.grid_size[0] and\n",
    "                0 <= grid_y < self.grid_size[1]):\n",
    "                grid[grid_x, grid_y, 0] = 1\n",
    "                grid[grid_x, grid_y, 1] = 0\n",
    "                grid[grid_x, grid_y, 2] = 0\n",
    "\n",
    "        # Process other vehicles\n",
    "        for vehicle in obs[1:]:\n",
    "            x, y, vx, vy, heading = vehicle\n",
    "\n",
    "            # Get relative position\n",
    "            rel_x = x - ego_x\n",
    "            rel_y = y - ego_y\n",
    "\n",
    "            # Get relative velocities\n",
    "            rel_vx = vx - ego_vx\n",
    "            rel_vy = vy - ego_vy\n",
    "\n",
    "            # Get relative heading\n",
    "\n",
    "            # Skip if vehicle center is out of range\n",
    "            if (rel_x < self.x_range[0] or rel_x > self.x_range[1] or\n",
    "                rel_y < self.y_range[0] or rel_y > self.y_range[1]):\n",
    "                continue\n",
    "\n",
    "            # Get all cells occupied by this vehicle\n",
    "            occupied_cells = self.get_car_footprint(rel_x, rel_y, heading)\n",
    "\n",
    "            # Convert to grid coordinates and update grid\n",
    "            for cell_x, cell_y in occupied_cells:\n",
    "                grid_x, grid_y = self.world_to_grid(cell_x, cell_y)\n",
    "\n",
    "                if (0 <= grid_x < self.grid_size[0] and\n",
    "                    0 <= grid_y < self.grid_size[1]):\n",
    "                    grid[grid_x, grid_y, 0] = 2\n",
    "                    grid[grid_x, grid_y, 1] = rel_vx\n",
    "                    grid[grid_x, grid_y, 2] = rel_vy\n",
    "\n",
    "        return np.array(grid).transpose(2,0,1)\n",
    "converter = KinematicToGridWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77318b7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:34.705619Z",
     "iopub.status.busy": "2024-12-31T02:30:34.705299Z",
     "iopub.status.idle": "2024-12-31T02:30:34.712496Z",
     "shell.execute_reply": "2024-12-31T02:30:34.711591Z"
    },
    "papermill": {
     "duration": 0.019342,
     "end_time": "2024-12-31T02:30:34.713907",
     "exception": false,
     "start_time": "2024-12-31T02:30:34.694565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def init_stack_of_planes(env, history_length=5):\n",
    "    \"\"\"\n",
    "    Initialize a stack of planes representing the grid and velocity information.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance, expected to have the method `unwrapped.observation_type.observe()`\n",
    "             to return the observation.\n",
    "        history_length (int): Number of past frames to stack.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (history_length + 2, grid_height, grid_width) representing the stack of planes.\n",
    "    \"\"\"\n",
    "    # Initialize the KinematicToGridWrapper\n",
    "    converter = KinematicToGridWrapper()\n",
    "\n",
    "    # Collect the initial observations and process each to form planes\n",
    "    grid_planes_list = []\n",
    "    for _ in range(history_length):\n",
    "        obs = env.unwrapped.observation_type.observe()\n",
    "        grid_planes = converter.process_observation(obs, -2, 4 * (4 - 1) + 2)[0]\n",
    "        grid_planes_list.append(grid_planes)\n",
    "\n",
    "    # Convert list to numpy array with shape (history_length, grid_height, grid_width)\n",
    "    stack_of_planes = np.stack(grid_planes_list, axis=0)\n",
    "\n",
    "    # Add two additional planes: obs[1] and obs[2] from the last observation\n",
    "    last_obs = converter.process_observation(env.unwrapped.observation_type.observe(), -2, 4 * (4 - 1) + 2)\n",
    "    plane_6 = last_obs[1]  # Second plane of the latest observation\n",
    "    plane_7 = last_obs[2]  # Third plane of the latest observation\n",
    "\n",
    "    # Concatenate the new planes to the stack\n",
    "    stack_of_planes = np.concatenate(\n",
    "        [stack_of_planes, np.expand_dims(plane_6, axis=0), np.expand_dims(plane_7, axis=0)], axis=0\n",
    "    )\n",
    "\n",
    "    return stack_of_planes\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_stack_of_planes(env, old_state, history_length=5):\n",
    "    \"\"\"\n",
    "    Update the stack of planes based on the latest observation.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance, expected to have the method `unwrapped.observation_type.observe()`\n",
    "             to return the observation.\n",
    "        old_state: A numpy array of shape (history_length + 2, grid_height, grid_width) representing the previous state.\n",
    "        history_length (int): Number of past frames to stack.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (history_length + 2, grid_height, grid_width) representing the updated stack of planes.\n",
    "    \"\"\"\n",
    "    # Initialize the KinematicToGridWrapper\n",
    "    converter = KinematicToGridWrapper()\n",
    "\n",
    "    # Get new observation and process it to form multiple planes\n",
    "    new_obs = converter.process_observation(env.unwrapped.observation_type.observe(), -2, 4 * (4 - 1) + 2)\n",
    "    new_obs_plane_0 = new_obs[0]  # First plane\n",
    "    new_obs_plane_1 = new_obs[1]  # Second plane\n",
    "    new_obs_plane_2 = new_obs[2]  # Third plane\n",
    "\n",
    "    # Shift old_state to remove the oldest frame and append the new one\n",
    "    stack_of_planes = np.roll(old_state, shift=-1, axis=0)\n",
    "    stack_of_planes[-3] = new_obs_plane_0  # Update the third-last plane\n",
    "    stack_of_planes[-2] = new_obs_plane_1  # Update the second-last plane\n",
    "    stack_of_planes[-1] = new_obs_plane_2  # Update the last plane\n",
    "\n",
    "    return stack_of_planes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16345c4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:34.735627Z",
     "iopub.status.busy": "2024-12-31T02:30:34.735324Z",
     "iopub.status.idle": "2024-12-31T02:30:34.808165Z",
     "shell.execute_reply": "2024-12-31T02:30:34.807208Z"
    },
    "papermill": {
     "duration": 0.085451,
     "end_time": "2024-12-31T02:30:34.809738",
     "exception": false,
     "start_time": "2024-12-31T02:30:34.724287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "duration = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b3607a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:34.831113Z",
     "iopub.status.busy": "2024-12-31T02:30:34.830821Z",
     "iopub.status.idle": "2024-12-31T02:30:34.835217Z",
     "shell.execute_reply": "2024-12-31T02:30:34.834582Z"
    },
    "papermill": {
     "duration": 0.016258,
     "end_time": "2024-12-31T02:30:34.836490",
     "exception": false,
     "start_time": "2024-12-31T02:30:34.820232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_policy(policy, available_actions):\n",
    "    \"\"\"\n",
    "    Áp dụng softmax cho các xác suất trong policy dựa trên available_actions.\n",
    "\n",
    "    :param policy: Dictionary chứa 5 hành động [0, 1, 2, 3, 4] với các xác suất tương ứng.\n",
    "    :param available_actions: Danh sách các hành động có thể thực hiện (subset của [0, 1, 2, 3, 4]).\n",
    "    :return: Dictionary chứa xác suất mới cho từng hành động (softmax áp dụng với các hành động khả dụng).\n",
    "    \"\"\"\n",
    "    # Lấy các giá trị xác suất tương ứng với available_actions\n",
    "    available_probs = np.array([policy[action] for action in available_actions])\n",
    "\n",
    "    # Áp dụng softmax chỉ trên available_probs\n",
    "    softmax_probs = available_probs / np.sum(available_probs)\n",
    "\n",
    "    # Cập nhật xác suất mới\n",
    "    updated_policy = {action: 0.0 for action in policy}  # Khởi tạo tất cả xác suất bằng 0\n",
    "    for action, prob in zip(available_actions, softmax_probs):\n",
    "        updated_policy[action] = prob\n",
    "\n",
    "    return updated_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8b5683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:34.858029Z",
     "iopub.status.busy": "2024-12-31T02:30:34.857717Z",
     "iopub.status.idle": "2024-12-31T02:30:36.473037Z",
     "shell.execute_reply": "2024-12-31T02:30:36.472224Z"
    },
    "papermill": {
     "duration": 1.628039,
     "end_time": "2024-12-31T02:30:36.474592",
     "exception": false,
     "start_time": "2024-12-31T02:30:34.846553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[133.61937 ,   4.      ,  25.      ,   0.      ,   0.      ],\n",
       "        [142.21725 ,   8.      ,  22.122345,   0.      ,   0.      ],\n",
       "        [151.6266  ,   0.      ,  22.895647,   0.      ,   0.      ],\n",
       "        [160.57358 ,   0.      ,  22.1782  ,   0.      ,   0.      ],\n",
       "        [168.86444 ,   0.      ,  21.171537,   0.      ,   0.      ],\n",
       "        [177.8137  ,   0.      ,  22.887634,   0.      ,   0.      ],\n",
       "        [187.06297 ,   8.      ,  23.109152,   0.      ,   0.      ],\n",
       "        [196.75107 ,   4.      ,  23.425312,   0.      ,   0.      ]],\n",
       "       dtype=float32),\n",
       " {'speed': 25,\n",
       "  'crashed': False,\n",
       "  'action': 3,\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'right_lane_reward': 0.3333333333333333,\n",
       "   'high_speed_reward': 0.5,\n",
       "   'on_road_reward': 1.0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 8,\n",
    "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\"],\n",
    "        \"absolute\": True,\n",
    "        \"normalize\": False,\n",
    "        \"order\": \"sorted\",\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "        \"target_speeds\": np.linspace(10, 30, 5)\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_density\": 1.6+np.random.rand(),\n",
    "}\n",
    "env = gym.make(\"highway-fast-v0\", config=config, render_mode='rgb_array')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c96f3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.497012Z",
     "iopub.status.busy": "2024-12-31T02:30:36.496633Z",
     "iopub.status.idle": "2024-12-31T02:30:36.506493Z",
     "shell.execute_reply": "2024-12-31T02:30:36.505855Z"
    },
    "papermill": {
     "duration": 0.021839,
     "end_time": "2024-12-31T02:30:36.507714",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.485875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, env, parent, parent_action, prior_prob, c_puct=2.5):\n",
    "        self.env = copy.deepcopy(env)\n",
    "        self.parent = parent #parent node\n",
    "        self.parent_action = parent_action\n",
    "        self.children = {} #parent.children[action] = child\n",
    "        self._n = 0\n",
    "        self._W = 0\n",
    "        self._P = prior_prob #Xác suất thực hiện hành động parent_action tại parent_node\n",
    "        self.c_puct = c_puct\n",
    "        min_speed = self.env.unwrapped.road.vehicles[0].target_speeds[0]\n",
    "        max_speed = self.env.unwrapped.road.vehicles[0].target_speeds[-1]\n",
    "        self.speed_bonus = (self.env.unwrapped.road.vehicles[0].speed - min_speed)/(max_speed - min_speed)\n",
    "        self.collision = 0\n",
    "        self.brake_penalty = 0\n",
    "        if self.parent_action==4:\n",
    "            self.brake_penalty = 1\n",
    "        if self.env.unwrapped.road.vehicles[0].crashed:\n",
    "            self.collision = 1 + 2*self.speed_bonus\n",
    "        if self.parent is None:\n",
    "            self.stack_of_planes = init_stack_of_planes(self.env)\n",
    "        else:\n",
    "            self.stack_of_planes = get_stack_of_planes(self.env, self.parent.stack_of_planes)\n",
    "    def pucb_score(self):\n",
    "        \"\"\"\n",
    "        Tính PUCB của node\n",
    "        \"\"\"\n",
    "        if self._n == 0:\n",
    "            Q = 0\n",
    "        else:\n",
    "            Q = self._W / self._n\n",
    "\n",
    "        return Q + self.c_puct * self._P * np.sqrt(self.parent._n) / (1 + self._n) + 0.5*self.speed_bonus - 0.4*self.collision - 0.2*self.brake_penalty\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Chọn node có UCB lớn nhất\n",
    "        \"\"\"\n",
    "        if not self.children:  # Nếu không có node con\n",
    "            return None  # Hoặc raise Exception(\"No children nodes to select from\")\n",
    "        return max(self.children.values(), key=lambda child: child.pucb_score())\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"\n",
    "        Mở rộng cây bằng cách tạo node con\n",
    "        action_priors là một dictionary chứa các xác suất prior của các action\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors.items():\n",
    "            if action not in self.children and prob>0:\n",
    "                #print(f\"expanded {action} with {prob}\")\n",
    "                copy_env = copy.deepcopy(self.env)\n",
    "                copy_env.step(action)\n",
    "                self.children[action] = MCTSNode(copy_env, self, action, prob)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Kiểm tra node có phải là leaf không\n",
    "        \"\"\"\n",
    "        return self.children == {}\n",
    "    def backpropagate(self, result):\n",
    "        \"\"\"\n",
    "        Cập nhật visit count n và tổng điểm W\n",
    "        new Q = new W/ new n\n",
    "        \"\"\"\n",
    "        self._n += 1\n",
    "        self._W += result\n",
    "    def backpropagate_recursive(self, result):\n",
    "        \"\"\"\n",
    "        Cập nhật toàn bộ đường đi từ node hiện tại đến root\n",
    "        \"\"\"\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate_recursive(result)\n",
    "        self.backpropagate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3044d666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.528310Z",
     "iopub.status.busy": "2024-12-31T02:30:36.528039Z",
     "iopub.status.idle": "2024-12-31T02:30:36.535657Z",
     "shell.execute_reply": "2024-12-31T02:30:36.535039Z"
    },
    "papermill": {
     "duration": 0.019259,
     "end_time": "2024-12-31T02:30:36.536859",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.517600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, root, network, c_puct=3.5, n_simulations=10, min_average_speed=23, duration=12):\n",
    "        self.c_puct = c_puct\n",
    "        self.root = root\n",
    "        self._network = network.to(device)\n",
    "        self._n_simulations = n_simulations\n",
    "        self.ego_init_position = root.env.unwrapped.road.vehicles[0].position[0]\n",
    "        self.min_average_speed = min_average_speed\n",
    "        self.duration = duration\n",
    "    def traverse_to_leaf(self):\n",
    "        node = self.root\n",
    "        while not node.is_leaf():\n",
    "            node = node.select()\n",
    "        return node\n",
    "\n",
    "    def rollout(self):\n",
    "        leaf_node = self.traverse_to_leaf()\n",
    "        truncated = leaf_node.env.unwrapped._is_truncated() # True nếu hoàn thành episode\n",
    "        crashed = leaf_node.env.unwrapped.road.vehicles[0].crashed # True nếu ego-vehicle xảy ra va chạm\n",
    "        leaf_state = leaf_node.stack_of_planes\n",
    "        state_tensor = torch.tensor(leaf_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        predicted_policy, predicted_value = self._network(state_tensor)\n",
    "        predicted_policy = {action: prob for action, prob in enumerate(predicted_policy.squeeze().tolist())}\n",
    "        #print(predicted_policy)\n",
    "        available_actions = leaf_node.env.unwrapped.get_available_actions()\n",
    "        #print(available_actions)\n",
    "        updated_policy = softmax_policy(predicted_policy, available_actions)\n",
    "        #print(updated_policy)\n",
    "        predicted_value = predicted_value.item()\n",
    "        if not truncated and not crashed:\n",
    "            leaf_node.expand(updated_policy)\n",
    "        elif truncated:\n",
    "            ego_last_position = leaf_node.env.unwrapped.road.vehicles[0].position[0]\n",
    "            ego_average_speed = (ego_last_position - self.ego_init_position)/(self.duration-1)\n",
    "            confidence_score = ego_average_speed / self.min_average_speed\n",
    "            if confidence_score >=1.0:\n",
    "                predicted_value = 1.0\n",
    "            else:\n",
    "                predicted_value = 0.0\n",
    "        elif crashed:\n",
    "            predicted_value = -1.0\n",
    "        leaf_node.backpropagate_recursive(predicted_value)\n",
    "\n",
    "    def move_to_new_root(self, action):\n",
    "        \"\"\"\n",
    "        Chuyển gốc của cây MCTS tới node con tương ứng với hành động được chọn.\n",
    "        \"\"\"\n",
    "        if action in self.root.children:\n",
    "            self.root = self.root.children[action]  # Di chuyển gốc tới node con\n",
    "            self.root.parent = None  # Ngắt liên kết với node cha để giảm bộ nhớ\n",
    "        else:\n",
    "            # Nếu node con không tồn tại, khởi tạo lại cây tại node gốc mới\n",
    "            raise ValueError(\"Hành động không có trong cây hiện tại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3728969c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.557637Z",
     "iopub.status.busy": "2024-12-31T02:30:36.557397Z",
     "iopub.status.idle": "2024-12-31T02:30:36.572655Z",
     "shell.execute_reply": "2024-12-31T02:30:36.571997Z"
    },
    "papermill": {
     "duration": 0.027311,
     "end_time": "2024-12-31T02:30:36.574119",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.546808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch\n",
    "\n",
    "class AlphaZeroTrainer:\n",
    "    def __init__(self, network, env, c_puct=2, n_simulations=10, learning_rate=0.001, batch_size=32, epochs=10):\n",
    "        self.network = network  # AlphaZeroNetwork\n",
    "        self.env = env\n",
    "        self.c_puct = c_puct\n",
    "        self.n_simulations = n_simulations\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "        self.training_data = []  # Lưu trữ dữ liệu huấn luyện dạng (state, policy, value, action)\n",
    "        self.action_list = []\n",
    "\n",
    "    def self_play(self, seed=21):\n",
    "        \"\"\"\n",
    "        Tạo dữ liệu huấn luyện thông qua self-play với MCTS.\n",
    "        \"\"\"\n",
    "        # Khởi tạo lại môi trường và trạng thái ban đầu\n",
    "        self.env.reset(seed=seed)\n",
    "        state = init_stack_of_planes(env)\n",
    "        done = self.env.unwrapped._is_truncated() or self.env.unwrapped._is_terminated()\n",
    "\n",
    "        # Tạo gốc của cây MCTS\n",
    "        root_node = MCTSNode(self.env, parent=None, parent_action=None, prior_prob=1.0, c_puct=self.c_puct)\n",
    "        mcts = MCTS(root=root_node, network=self.network, c_puct=self.c_puct, n_simulations=self.n_simulations, duration=duration)\n",
    "\n",
    "        while not done:\n",
    "            # Thực hiện MCTS rollout để tính xác suất hành động\n",
    "            state = get_stack_of_planes(env, state)\n",
    "            for _ in range(self.n_simulations):\n",
    "                mcts.rollout()\n",
    "            # Thu thập xác suất hành động và giá trị của trạng thái hiện tại\n",
    "            action_probs = {action: 0.0 for action in range(5)}  # Khởi tạo xác suất của tất cả hành động là 0\n",
    "            for action, child in root_node.children.items():\n",
    "                action_probs[action] = child._n / (root_node._n - 1)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            #print(state_tensor.shape)\n",
    "            predicted_value = root_node._W / root_node._n if root_node._n > 0 else 0\n",
    "\n",
    "            # Lưu trữ dữ liệu huấn luyện\n",
    "\n",
    "\n",
    "            # Chọn hành động dựa trên xác suất từ MCTS\n",
    "            action = max(action_probs, key=action_probs.get)\n",
    "            self.action_list.append(action)\n",
    "            self.env.step(action)\n",
    "            #print(f\"action chosen: {action}\")\n",
    "            self.training_data.append((state_tensor, action_probs, predicted_value, action))\n",
    "            #(env.unwrapped.road.vehicles[0].target_lane_index[2])\n",
    "\n",
    "            # Di chuyển gốc của MCTS đến node con tương ứng với hành động được chọn\n",
    "            if action in root_node.children:\n",
    "                mcts.move_to_new_root(action)\n",
    "                root_node = mcts.root  # Cập nhật root_node cho vòng lặp kế tiếp\n",
    "            else:\n",
    "                raise ValueError(\"Action không tồn tại trong cây MCTS.\")\n",
    "\n",
    "            # Cập nhật trạng thái và kiểm tra điều kiện kết thúc\n",
    "            done = self.env.unwrapped._is_truncated() or self.env.unwrapped._is_terminated()\n",
    "        print(\"end self-play\")\n",
    "\n",
    "    def train(self):\n",
    "        self.network.to(device)\n",
    "        states, policies, values, actions = zip(*self.training_data)\n",
    "        states = torch.cat(states).to(device)\n",
    "        policies = torch.tensor([list(policy.values()) for policy in policies], dtype=torch.float32).to(device)\n",
    "        values = torch.tensor(values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "\n",
    "        # Weighted sampling\n",
    "        class_counts = torch.bincount(actions)\n",
    "        class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "        sample_weights = class_weights[actions]\n",
    "\n",
    "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "        dataset = TensorDataset(states, policies, values, actions)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, sampler=sampler)\n",
    "\n",
    "        # Khởi tạo các danh sách lưu trữ loss\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.total_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_policy_loss = 0\n",
    "            epoch_value_loss = 0\n",
    "            epoch_total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for state_batch, policy_batch, value_batch, action_batch in dataloader:\n",
    "                # Move batches to device\n",
    "                state_batch = state_batch.to(device)\n",
    "                policy_batch = policy_batch.to(device)\n",
    "                value_batch = value_batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                predicted_policy, predicted_value = self.network(state_batch)\n",
    "\n",
    "                # Losses\n",
    "                policy_loss = F.kl_div(\n",
    "                    F.log_softmax(predicted_policy, dim=-1),\n",
    "                    policy_batch,\n",
    "                    reduction='batchmean'\n",
    "                )\n",
    "                #policy_loss = F.cross_entropy(predicted_policy, policy_batch)\n",
    "                value_loss = F.mse_loss(predicted_value, value_batch)\n",
    "                loss = 0.9*policy_loss + 0.1*value_loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Cộng dồn loss\n",
    "                epoch_policy_loss += policy_loss.item()\n",
    "                epoch_value_loss += value_loss.item()\n",
    "                epoch_total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            # Tính loss trung bình cho mỗi epoch\n",
    "            avg_policy_loss = epoch_policy_loss / batch_count\n",
    "            avg_value_loss = epoch_value_loss / batch_count\n",
    "            avg_total_loss = epoch_total_loss / batch_count\n",
    "\n",
    "            # Lưu loss vào danh sách\n",
    "            self.policy_losses.append(avg_policy_loss)\n",
    "            self.value_losses.append(avg_value_loss)\n",
    "            self.total_losses.append(avg_total_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, value loss: {avg_value_loss}, policy loss: {avg_policy_loss}, Loss: {avg_total_loss}\")\n",
    "            self.scheduler.step(avg_total_loss)\n",
    "\n",
    "\n",
    "    def save_model(self, path=\"alphazero_model.pth\"):\n",
    "        torch.save(self.network.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path=\"alphazero_model.pth\"):\n",
    "        self.network.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0ee05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.594946Z",
     "iopub.status.busy": "2024-12-31T02:30:36.594689Z",
     "iopub.status.idle": "2024-12-31T02:30:36.599113Z",
     "shell.execute_reply": "2024-12-31T02:30:36.598465Z"
    },
    "papermill": {
     "duration": 0.0163,
     "end_time": "2024-12-31T02:30:36.600575",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.584275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def env_init(duration):\n",
    "    config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 8,\n",
    "            \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\"],\n",
    "            \"absolute\": True,\n",
    "            \"normalize\": False,\n",
    "            \"order\": \"sorted\",\n",
    "        },\n",
    "        \"action\": {\n",
    "            \"type\": \"DiscreteMetaAction\",\n",
    "            \"target_speeds\": np.linspace(10, 30, 5)\n",
    "        },\n",
    "        \"lanes_count\": 4,\n",
    "        \"vehicles_density\": 1.6+np.random.rand(),\n",
    "        \"duration\": duration,\n",
    "    }\n",
    "    env = gym.make(\"highway-fast-v0\", config=config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abcb9457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.621928Z",
     "iopub.status.busy": "2024-12-31T02:30:36.621665Z",
     "iopub.status.idle": "2024-12-31T02:30:36.643024Z",
     "shell.execute_reply": "2024-12-31T02:30:36.642413Z"
    },
    "papermill": {
     "duration": 0.033995,
     "end_time": "2024-12-31T02:30:36.644474",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.610479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = env_init(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6748b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.666422Z",
     "iopub.status.busy": "2024-12-31T02:30:36.666155Z",
     "iopub.status.idle": "2024-12-31T02:30:36.796087Z",
     "shell.execute_reply": "2024-12-31T02:30:36.795392Z"
    },
    "papermill": {
     "duration": 0.142241,
     "end_time": "2024-12-31T02:30:36.797773",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.655532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "network = AlphaZeroNetwork(input_shape=(120,20,7), n_residual_layers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee97eb12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:36.820309Z",
     "iopub.status.busy": "2024-12-31T02:30:36.819973Z",
     "iopub.status.idle": "2024-12-31T02:30:38.015793Z",
     "shell.execute_reply": "2024-12-31T02:30:38.014842Z"
    },
    "papermill": {
     "duration": 1.208075,
     "end_time": "2024-12-31T02:30:38.017294",
     "exception": false,
     "start_time": "2024-12-31T02:30:36.809219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = AlphaZeroTrainer(network, env, c_puct=2.0, n_simulations=150, learning_rate=0.001, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7575aa11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:38.038177Z",
     "iopub.status.busy": "2024-12-31T02:30:38.037763Z",
     "iopub.status.idle": "2024-12-31T02:30:39.148407Z",
     "shell.execute_reply": "2024-12-31T02:30:39.147703Z"
    },
    "papermill": {
     "duration": 1.122479,
     "end_time": "2024-12-31T02:30:39.149983",
     "exception": false,
     "start_time": "2024-12-31T02:30:38.027504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-b613e12249ca>:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.network.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "trainer.load_model('/kaggle/input/alphazero/pytorch/ver0/1/alphazero_model (25).pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adae1b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:39.171436Z",
     "iopub.status.busy": "2024-12-31T02:30:39.171181Z",
     "iopub.status.idle": "2024-12-31T02:30:39.174181Z",
     "shell.execute_reply": "2024-12-31T02:30:39.173516Z"
    },
    "papermill": {
     "duration": 0.014791,
     "end_time": "2024-12-31T02:30:39.175460",
     "exception": false,
     "start_time": "2024-12-31T02:30:39.160669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluate(network, seed):\n",
    "#     action_list = []\n",
    "#     speed_list = []\n",
    "#     env = env_init(40)\n",
    "#     state = init_stack_of_planes(env)\n",
    "#     trainer = AlphaZeroTrainer(network, env, c_puct=3.5, n_simulations=15, learning_rate=0.001, batch_size=64, epochs=30)\n",
    "#     trainer.network.eval()\n",
    "#     ego_position_list = []\n",
    "#     ego_position_list.append(env.unwrapped.road.vehicles[0].position[0])\n",
    "#     while not env.unwrapped._is_terminated() and not env.unwrapped._is_truncated():\n",
    "#         obs = env.unwrapped.observation_type.observe()\n",
    "#         state = get_stack_of_planes(env, state)\n",
    "#         state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "#         predicted_policy, predicted_value = trainer.network(state_tensor.to(device))\n",
    "#         available_actions = env.unwrapped.get_available_actions()\n",
    "#         predicted_policy = {action: prob for action, prob in enumerate(predicted_policy.squeeze().tolist())}\n",
    "#         updated_policy = softmax_policy(predicted_policy, available_actions)\n",
    "#         action = max(updated_policy, key=updated_policy.get)\n",
    "#         env.render()\n",
    "#         print(f\"action chosen: {action}\")\n",
    "#         env.step(action)\n",
    "\n",
    "# for seed in range(0, 20):\n",
    "#     evaluate(network=network ,seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e23e8ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:39.196228Z",
     "iopub.status.busy": "2024-12-31T02:30:39.195928Z",
     "iopub.status.idle": "2024-12-31T02:30:39.200529Z",
     "shell.execute_reply": "2024-12-31T02:30:39.199834Z"
    },
    "papermill": {
     "duration": 0.016224,
     "end_time": "2024-12-31T02:30:39.201787",
     "exception": false,
     "start_time": "2024-12-31T02:30:39.185563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def training_pipeline(trainer, iterations=3, init_duration=20, duration_increase=5):\n",
    "    trainer.network.train()\n",
    "    for i in range(iterations):\n",
    "        trainer.training_data = []\n",
    "        env = env_init(duration=init_duration + duration_increase*i)\n",
    "        trainer.env = env\n",
    "        print(f\"iter: {i}\")\n",
    "        j = 0\n",
    "        while len(trainer.training_data)<2500:\n",
    "            print(f\"self-play: {j}\")\n",
    "            trainer.self_play(seed=j)\n",
    "            j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f8fdd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:39.223722Z",
     "iopub.status.busy": "2024-12-31T02:30:39.223401Z",
     "iopub.status.idle": "2024-12-31T02:30:39.226503Z",
     "shell.execute_reply": "2024-12-31T02:30:39.225835Z"
    },
    "papermill": {
     "duration": 0.015624,
     "end_time": "2024-12-31T02:30:39.227772",
     "exception": false,
     "start_time": "2024-12-31T02:30:39.212148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_pipeline(trainer, iterations=1, init_duration=20, duration_increase=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c53ce564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:39.249654Z",
     "iopub.status.busy": "2024-12-31T02:30:39.249324Z",
     "iopub.status.idle": "2024-12-31T02:30:39.252523Z",
     "shell.execute_reply": "2024-12-31T02:30:39.251832Z"
    },
    "papermill": {
     "duration": 0.016074,
     "end_time": "2024-12-31T02:30:39.253979",
     "exception": false,
     "start_time": "2024-12-31T02:30:39.237905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_data = trainer.training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4d698b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:39.276285Z",
     "iopub.status.busy": "2024-12-31T02:30:39.275924Z",
     "iopub.status.idle": "2024-12-31T02:30:39.278943Z",
     "shell.execute_reply": "2024-12-31T02:30:39.278252Z"
    },
    "papermill": {
     "duration": 0.015527,
     "end_time": "2024-12-31T02:30:39.280314",
     "exception": false,
     "start_time": "2024-12-31T02:30:39.264787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"training_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(training_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10370d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:39.303082Z",
     "iopub.status.busy": "2024-12-31T02:30:39.302767Z",
     "iopub.status.idle": "2024-12-31T02:30:45.621077Z",
     "shell.execute_reply": "2024-12-31T02:30:45.620054Z"
    },
    "papermill": {
     "duration": 6.331762,
     "end_time": "2024-12-31T02:30:45.623276",
     "exception": false,
     "start_time": "2024-12-31T02:30:39.291514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/kaggle/input/training-data-10000-samples/training_data (1).pkl\", \"rb\") as f:\n",
    "    training_data1 = pickle.load(f)\n",
    "with open(\"/kaggle/input/training-data-10000-samples/training_data.pkl\", \"rb\") as f:\n",
    "    training_data2 = pickle.load(f)\n",
    "with open(\"/kaggle/input/training-data-10000-samples/training_data (7).pkl\", \"rb\") as f:\n",
    "    training_data3 = pickle.load(f)\n",
    "with open(\"/kaggle/input/training-data-10000-samples/training_data (8).pkl\", \"rb\") as f:\n",
    "    training_data4 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee1ac56f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:45.645059Z",
     "iopub.status.busy": "2024-12-31T02:30:45.644798Z",
     "iopub.status.idle": "2024-12-31T02:30:45.649816Z",
     "shell.execute_reply": "2024-12-31T02:30:45.649004Z"
    },
    "papermill": {
     "duration": 0.017448,
     "end_time": "2024-12-31T02:30:45.651304",
     "exception": false,
     "start_time": "2024-12-31T02:30:45.633856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10008"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = training_data1+training_data2+training_data3+training_data4\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1b5566c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:45.672211Z",
     "iopub.status.busy": "2024-12-31T02:30:45.671940Z",
     "iopub.status.idle": "2024-12-31T02:30:45.674970Z",
     "shell.execute_reply": "2024-12-31T02:30:45.674368Z"
    },
    "papermill": {
     "duration": 0.014685,
     "end_time": "2024-12-31T02:30:45.676170",
     "exception": false,
     "start_time": "2024-12-31T02:30:45.661485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.training_data = training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "938858f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T02:30:45.696739Z",
     "iopub.status.busy": "2024-12-31T02:30:45.696508Z",
     "iopub.status.idle": "2024-12-31T10:22:00.733744Z",
     "shell.execute_reply": "2024-12-31T10:22:00.732754Z"
    },
    "papermill": {
     "duration": 28275.061717,
     "end_time": "2024-12-31T10:22:00.747883",
     "exception": false,
     "start_time": "2024-12-31T02:30:45.686166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, value loss: 0.2422394486749248, policy loss: 0.729133094571958, Loss: 0.6804437147583932\n",
      "Epoch 2/50, value loss: 0.2472449842911617, policy loss: 0.7347677399398415, Loss: 0.6860154479931874\n",
      "Epoch 3/50, value loss: 0.2442529568816446, policy loss: 0.7269720951463007, Loss: 0.6787001657637821\n",
      "Epoch 4/50, value loss: 0.24280762568021277, policy loss: 0.7224093144107017, Loss: 0.6744491271911912\n",
      "Epoch 5/50, value loss: 0.24379478167196747, policy loss: 0.7254879394913935, Loss: 0.677318605647725\n",
      "Epoch 6/50, value loss: 0.248140097138988, policy loss: 0.7348231043025946, Loss: 0.686154788087128\n",
      "Epoch 7/50, value loss: 0.2495961639152211, policy loss: 0.7308000899424218, Loss: 0.6826796797430439\n",
      "Epoch 8/50, value loss: 0.24199747934842566, policy loss: 0.7204470854655952, Loss: 0.6726021068111346\n",
      "Epoch 9/50, value loss: 0.24131480666102878, policy loss: 0.7214051997585661, Loss: 0.6733961390082244\n",
      "Epoch 10/50, value loss: 0.24281564734543964, policy loss: 0.7254589442994185, Loss: 0.6771945987537409\n",
      "Epoch 11/50, value loss: 0.24468015115352193, policy loss: 0.7197807374273896, Loss: 0.6722706605674355\n",
      "Epoch 12/50, value loss: 0.24838071140893705, policy loss: 0.7191536232924006, Loss: 0.6720763175350846\n",
      "Epoch 13/50, value loss: 0.245274205970916, policy loss: 0.7226491517322079, Loss: 0.6749116413912196\n",
      "Epoch 14/50, value loss: 0.2462285713405366, policy loss: 0.722762377778436, Loss: 0.6751089783231164\n",
      "Epoch 15/50, value loss: 0.24373224121370132, policy loss: 0.7296407423960934, Loss: 0.6810498750133879\n",
      "Epoch 16/50, value loss: 0.2417846465376532, policy loss: 0.7222508867834784, Loss: 0.6742042458740769\n",
      "Epoch 17/50, value loss: 0.24406065359996382, policy loss: 0.7199411251742369, Loss: 0.6723530619007767\n",
      "Epoch 18/50, value loss: 0.24621201026591527, policy loss: 0.7304082129411636, Loss: 0.6819885729984113\n",
      "Epoch 19/50, value loss: 0.24672111973261376, policy loss: 0.7241977662037892, Loss: 0.6764500870066843\n",
      "Epoch 20/50, value loss: 0.24347451129916367, policy loss: 0.7246684025806985, Loss: 0.6765489954098015\n",
      "Epoch 21/50, value loss: 0.24558633860129458, policy loss: 0.7312073715173515, Loss: 0.682645252935446\n",
      "Epoch 22/50, value loss: 0.2437932747564498, policy loss: 0.7346229276080041, Loss: 0.6855399464346041\n",
      "Epoch 23/50, value loss: 0.24442744758098747, policy loss: 0.7289638268719812, Loss: 0.6805101731780229\n",
      "Epoch 24/50, value loss: 0.24781943421075298, policy loss: 0.7289892902040178, Loss: 0.6808722877198723\n",
      "Epoch 25/50, value loss: 0.24079607559996805, policy loss: 0.728276286155555, Loss: 0.679528248917525\n",
      "Epoch 26/50, value loss: 0.2456857380783482, policy loss: 0.7278878480006176, Loss: 0.6796676186239643\n",
      "Epoch 27/50, value loss: 0.24220099285909324, policy loss: 0.7293254922909341, Loss: 0.6806130249788807\n",
      "Epoch 28/50, value loss: 0.2403014210189224, policy loss: 0.7253143908871207, Loss: 0.6768130796730139\n",
      "Epoch 29/50, value loss: 0.2393462391226155, policy loss: 0.7191622283807986, Loss: 0.6711806138609625\n",
      "Epoch 30/50, value loss: 0.24491825870647552, policy loss: 0.7345207075404513, Loss: 0.6855604439784008\n",
      "Epoch 31/50, value loss: 0.2482583763872742, policy loss: 0.7230254434476233, Loss: 0.6755487186134241\n",
      "Epoch 32/50, value loss: 0.2433970100750589, policy loss: 0.7207212375987108, Loss: 0.6729887997268871\n",
      "Epoch 33/50, value loss: 0.24242756795731318, policy loss: 0.7277035963763098, Loss: 0.6791759771146592\n",
      "Epoch 34/50, value loss: 0.2418299249023389, policy loss: 0.7261028217661912, Loss: 0.6776755148438132\n",
      "Epoch 35/50, value loss: 0.24098903500730065, policy loss: 0.7297390767723132, Loss: 0.6808640577231243\n",
      "Epoch 36/50, value loss: 0.24433035673988854, policy loss: 0.7283910687562007, Loss: 0.6799849799484204\n",
      "Epoch 37/50, value loss: 0.24227611530738272, policy loss: 0.7172472101108284, Loss: 0.669750084163277\n",
      "Epoch 38/50, value loss: 0.24041712378999988, policy loss: 0.7270828250107492, Loss: 0.6784162377096286\n",
      "Epoch 39/50, value loss: 0.24644998256947584, policy loss: 0.7258785135427098, Loss: 0.67793564318092\n",
      "Epoch 40/50, value loss: 0.24155188926086305, policy loss: 0.7219322673074758, Loss: 0.673894212124454\n",
      "Epoch 41/50, value loss: 0.24323104260263928, policy loss: 0.7286787044470477, Loss: 0.6801339213255864\n",
      "Epoch 42/50, value loss: 0.24718599609888284, policy loss: 0.7216158038491656, Loss: 0.6741728042341342\n",
      "Epoch 43/50, value loss: 0.2386537485631408, policy loss: 0.7207645610639244, Loss: 0.6725534641059341\n",
      "Epoch 44/50, value loss: 0.24196270535326309, policy loss: 0.7240791533403336, Loss: 0.6758674941245159\n",
      "Epoch 45/50, value loss: 0.24397455544988061, policy loss: 0.7369380050404056, Loss: 0.687641643035184\n",
      "Epoch 46/50, value loss: 0.2415136992931366, policy loss: 0.7210573728676815, Loss: 0.6731029893182645\n",
      "Epoch 47/50, value loss: 0.24969697625014434, policy loss: 0.7275751320419798, Loss: 0.6797872998152569\n",
      "Epoch 48/50, value loss: 0.24508664106867115, policy loss: 0.7276795886124775, Loss: 0.6794202775712226\n",
      "Epoch 49/50, value loss: 0.246874764656565, policy loss: 0.7289570011910359, Loss: 0.6807487588019887\n",
      "Epoch 50/50, value loss: 0.24441935397257472, policy loss: 0.7197653389280769, Loss: 0.6722307231775515\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d80cf8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T10:22:00.774526Z",
     "iopub.status.busy": "2024-12-31T10:22:00.774209Z",
     "iopub.status.idle": "2024-12-31T10:22:00.843346Z",
     "shell.execute_reply": "2024-12-31T10:22:00.842308Z"
    },
    "papermill": {
     "duration": 0.084261,
     "end_time": "2024-12-31T10:22:00.844953",
     "exception": false,
     "start_time": "2024-12-31T10:22:00.760692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6391527,
     "sourceId": 10323173,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6391560,
     "sourceId": 10323254,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 205386,
     "modelInstanceId": 183185,
     "sourceId": 214868,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28358.701734,
   "end_time": "2024-12-31T10:22:02.789629",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-31T02:29:24.087895",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
