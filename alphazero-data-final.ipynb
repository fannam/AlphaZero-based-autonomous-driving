{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e86b2e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-28T13:47:09.416278Z",
     "iopub.status.busy": "2024-12-28T13:47:09.415912Z",
     "iopub.status.idle": "2024-12-28T13:48:18.383572Z",
     "shell.execute_reply": "2024-12-28T13:48:18.382329Z"
    },
    "papermill": {
     "duration": 68.975063,
     "end_time": "2024-12-28T13:48:18.385392",
     "exception": false,
     "start_time": "2024-12-28T13:47:09.410329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting highway-env\r\n",
      "  Downloading highway_env-1.10.1-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.0)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\r\n",
      "Collecting gymnasium\r\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from highway-env) (0.0.4)\r\n",
      "Requirement already satisfied: pygame>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from highway-env) (2.6.0)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from highway-env) (3.7.1)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from highway-env) (2.1.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from highway-env) (1.13.1)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (24.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->highway-env) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway-env) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->highway-env) (2024.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->highway-env) (1.16.0)\r\n",
      "Downloading highway_env-1.10.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: gymnasium, highway-env\r\n",
      "  Attempting uninstall: gymnasium\r\n",
      "    Found existing installation: gymnasium 0.29.0\r\n",
      "    Uninstalling gymnasium-0.29.0:\r\n",
      "      Successfully uninstalled gymnasium-0.29.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "kaggle-environments 1.16.10 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\r\n",
      "stable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed gymnasium-1.0.0 highway-env-1.10.1\r\n",
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\r\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\r\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\r\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\r\n",
      "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\r\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,196 kB]\r\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\r\n",
      "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,631 kB]\r\n",
      "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\r\n",
      "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\r\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\r\n",
      "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\r\n",
      "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,563 kB]\r\n",
      "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\r\n",
      "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\r\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\r\n",
      "Get:17 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [53.4 kB]\r\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\r\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\r\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,633 kB]\r\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,840 kB]\r\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\r\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\r\n",
      "Fetched 28.2 MB in 4s (7,752 kB/s)\r\n",
      "\r\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "build-essential is already the newest version (12.9ubuntu3).\r\n",
      "Suggested packages:\r\n",
      "  swig-doc swig-examples swig4.0-examples swig4.0-doc\r\n",
      "The following NEW packages will be installed:\r\n",
      "  swig swig4.0\r\n",
      "0 upgraded, 2 newly installed, 0 to remove and 136 not upgraded.\r\n",
      "Need to get 1,116 kB of archives.\r\n",
      "After this operation, 5,542 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\r\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\r\n",
      "Fetched 1,116 kB in 1s (1,758 kB/s)\r\n",
      "debconf: unable to initialize frontend: Dialog\r\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\r\n",
      "debconf: falling back to frontend: Readline\r\n",
      "Selecting previously unselected package swig4.0.\r\n",
      "(Reading database ... 127365 files and directories currently installed.)\r\n",
      "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\r\n",
      "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\r\n",
      "Selecting previously unselected package swig.\r\n",
      "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\r\n",
      "Unpacking swig (4.0.2-1ubuntu1) ...\r\n",
      "Setting up swig4.0 (4.0.2-1ubuntu1) ...\r\n",
      "Setting up swig (4.0.2-1ubuntu1) ...\r\n",
      "Processing triggers for man-db (2.10.2-1) ...\r\n",
      "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\r\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\r\n",
      "Collecting swig==4.* (from gymnasium[box2d])\r\n",
      "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\r\n",
      "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: box2d-py\r\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349139 sha256=d54b7d5d004fd799b24b7922150d545d80f14629919a55ebebcffb7050bb78f9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\r\n",
      "Successfully built box2d-py\r\n",
      "Installing collected packages: swig, box2d-py\r\n",
      "Successfully installed box2d-py-2.3.5 swig-4.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install highway-env gymnasium torch numpy tqdm\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install build-essential swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e90b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:18.407356Z",
     "iopub.status.busy": "2024-12-28T13:48:18.407069Z",
     "iopub.status.idle": "2024-12-28T13:48:23.365667Z",
     "shell.execute_reply": "2024-12-28T13:48:23.364623Z"
    },
    "papermill": {
     "duration": 4.971344,
     "end_time": "2024-12-28T13:48:23.367220",
     "exception": false,
     "start_time": "2024-12-28T13:48:18.395876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy shape: torch.Size([8, 5])\n",
      "Value shape: torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetRNNPolicyValue(nn.Module):\n",
    "    def __init__(self, num_timesteps=5, num_actions=5):\n",
    "        super(ResNetRNNPolicyValue, self).__init__()\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # Load ResNet and modify for single-channel input\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Change input channels to 1\n",
    "        resnet.fc = nn.Identity()  # Remove the fully connected layer\n",
    "\n",
    "        self.resnet = resnet\n",
    "\n",
    "        # Calculate output size after ResNet\n",
    "        self.flatten_dim = 512  # Output of ResNet backbone\n",
    "\n",
    "        # RNN Module\n",
    "        self.rnn = nn.LSTM(input_size=self.flatten_dim, hidden_size=128, num_layers=1, batch_first=True)\n",
    "\n",
    "        # Policy Head\n",
    "        self.fc_policy = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_actions),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        # Value Head\n",
    "        self.fc_value = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, height, width = x.size()\n",
    "\n",
    "        # Reshape to process each time step with ResNet\n",
    "        x = x.view(batch_size * timesteps, 1, height, width)  # Add channel dimension (1)\n",
    "\n",
    "        # Extract spatial features with ResNet\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(batch_size, timesteps, -1)  # Flatten spatial dimensions\n",
    "\n",
    "        # Learn temporal features with RNN\n",
    "        x, _ = self.rnn(x)\n",
    "\n",
    "        # Use the last time step output for predictions\n",
    "        x_last = x[:, -1, :]\n",
    "\n",
    "        # Policy and value predictions\n",
    "        policy = self.fc_policy(x_last)\n",
    "        value = self.fc_value(x_last)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = ResNetRNNPolicyValue(num_timesteps=5, num_actions=5)\n",
    "    sample_input = torch.randn(8, 5, 120, 20)  # Batch size 8, 5 timesteps, grid size 120x20\n",
    "    policy, value = model(sample_input)\n",
    "    print(\"Policy shape:\", policy.shape)  # Expected: [8, 5]\n",
    "    print(\"Value shape:\", value.shape)    # Expected: [8, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc25570b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:23.391161Z",
     "iopub.status.busy": "2024-12-28T13:48:23.390824Z",
     "iopub.status.idle": "2024-12-28T13:48:23.407791Z",
     "shell.execute_reply": "2024-12-28T13:48:23.407105Z"
    },
    "papermill": {
     "duration": 0.030924,
     "end_time": "2024-12-28T13:48:23.409101",
     "exception": false,
     "start_time": "2024-12-28T13:48:23.378177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KinematicToGridWrapper:\n",
    "    def __init__(self):\n",
    "        # Define grid parameters\n",
    "        self.x_range = (-30, 90)  # meters relative to ego\n",
    "        self.y_range = (-10, 10)   # meters relative to ego\n",
    "\n",
    "        # Calculate grid size based on 1m per cell\n",
    "        self.grid_size = (\n",
    "            self.x_range[1] - self.x_range[0],  # 120 cells for x\n",
    "            self.y_range[1] - self.y_range[0]   # 20 cells for y\n",
    "        )\n",
    "\n",
    "        # Car dimensions\n",
    "        self.car_length = 5  # meters\n",
    "        self.car_width = 2   # meters\n",
    "\n",
    "    def get_car_footprint(self, x, y, heading):\n",
    "        \"\"\"Calculate which cells a car occupies given its center and heading\"\"\"\n",
    "        occupied_cells = []\n",
    "\n",
    "        cos_h = np.cos(heading)\n",
    "        sin_h = np.sin(heading)\n",
    "\n",
    "        corners_car = [\n",
    "            (-self.car_length/2, -self.car_width/2),\n",
    "            (self.car_length/2, -self.car_width/2),\n",
    "            (self.car_length/2, self.car_width/2),\n",
    "            (-self.car_length/2, self.car_width/2)\n",
    "        ]\n",
    "\n",
    "        corners_world = [\n",
    "            (x + dx*cos_h - dy*sin_h, y + dx*sin_h + dy*cos_h)\n",
    "            for dx, dy in corners_car\n",
    "        ]\n",
    "\n",
    "        min_x = min(x[0] for x in corners_world)\n",
    "        max_x = max(x[0] for x in corners_world)\n",
    "        min_y = min(x[1] for x in corners_world)\n",
    "        max_y = max(x[1] for x in corners_world)\n",
    "\n",
    "        for cell_x in range(int(min_x), int(max_x) + 1):\n",
    "            for cell_y in range(int(min_y), int(max_y) + 1):\n",
    "                if self.point_in_rotated_rect(\n",
    "                    cell_x + 0.5, cell_y + 0.5,\n",
    "                    x, y, heading,\n",
    "                    self.car_length, self.car_width\n",
    "                ):\n",
    "                    occupied_cells.append((cell_x, cell_y))\n",
    "\n",
    "        return occupied_cells\n",
    "\n",
    "    def world_to_grid(self, x, y):\n",
    "        \"\"\"Convert world coordinates to grid coordinates\"\"\"\n",
    "        grid_x = int(x - self.x_range[0])\n",
    "        # Flip y-axis to maintain correct orientation\n",
    "        grid_y = int(self.grid_size[1] - (y - self.y_range[0]) - 1)\n",
    "        return grid_x, grid_y\n",
    "\n",
    "    def point_in_rotated_rect(self, px, py, rect_x, rect_y, rect_angle, length, width):\n",
    "        dx = px - rect_x\n",
    "        dy = py - rect_y\n",
    "\n",
    "        cos_h = np.cos(-rect_angle)\n",
    "        sin_h = np.sin(-rect_angle)\n",
    "\n",
    "        rotated_x = dx * cos_h - dy * sin_h\n",
    "        rotated_y = dx * sin_h + dy * cos_h\n",
    "\n",
    "        return (abs(rotated_x) <= length/2) and (abs(rotated_y) <= width/2)\n",
    "\n",
    "    def process_observation(self, obs, left_bound, right_bound):\n",
    "        \"\"\"\n",
    "        Process vehicle observations and return separate ego info and occupancy grid\n",
    "        obs: list of [x, y, vx, vy, heading] for each vehicle (ego first)\n",
    "        \"\"\"\n",
    "        # Extract ego vehicle state\n",
    "        ego_x, ego_y, ego_vx, ego_vy, ego_heading = obs[0]\n",
    "\n",
    "        # Initialize grid\n",
    "        grid = np.zeros((self.grid_size[0], self.grid_size[1], 3), dtype=np.float32)\n",
    "\n",
    "        to_left =  ego_y - left_bound\n",
    "        to_right = right_bound - ego_y\n",
    "\n",
    "        left = int(self.grid_size[1]/2 -1 - to_left)\n",
    "        right = int(self.grid_size[1]/2 +1 + to_right)\n",
    "\n",
    "        if left >= 0:\n",
    "            grid[:, :left + 1, 0] = 2\n",
    "            grid[:, :left + 1, 2] = ego_vy\n",
    "        if right < self.grid_size[1]:\n",
    "            grid[:, right:, 0] = 2\n",
    "            grid[:, right:, 2] = ego_vy\n",
    "\n",
    "        # Place ego vehicle\n",
    "        ego_cells = self.get_car_footprint(0, 0, ego_heading)\n",
    "        for cell_x, cell_y in ego_cells:\n",
    "            grid_x, grid_y = self.world_to_grid(cell_x, cell_y)\n",
    "\n",
    "            if (0 <= grid_x < self.grid_size[0] and\n",
    "                0 <= grid_y < self.grid_size[1]):\n",
    "                grid[grid_x, grid_y, 0] = 1\n",
    "                grid[grid_x, grid_y, 1] = 0\n",
    "                grid[grid_x, grid_y, 2] = 0\n",
    "\n",
    "        # Process other vehicles\n",
    "        for vehicle in obs[1:]:\n",
    "            x, y, vx, vy, heading = vehicle\n",
    "\n",
    "            # Get relative position\n",
    "            rel_x = x - ego_x\n",
    "            rel_y = y - ego_y\n",
    "\n",
    "            # Get relative velocities\n",
    "            rel_vx = vx - ego_vx\n",
    "            rel_vy = vy - ego_vy\n",
    "\n",
    "            # Get relative heading\n",
    "\n",
    "            # Skip if vehicle center is out of range\n",
    "            if (rel_x < self.x_range[0] or rel_x > self.x_range[1] or\n",
    "                rel_y < self.y_range[0] or rel_y > self.y_range[1]):\n",
    "                continue\n",
    "\n",
    "            # Get all cells occupied by this vehicle\n",
    "            occupied_cells = self.get_car_footprint(rel_x, rel_y, heading)\n",
    "\n",
    "            # Convert to grid coordinates and update grid\n",
    "            for cell_x, cell_y in occupied_cells:\n",
    "                grid_x, grid_y = self.world_to_grid(cell_x, cell_y)\n",
    "\n",
    "                if (0 <= grid_x < self.grid_size[0] and\n",
    "                    0 <= grid_y < self.grid_size[1]):\n",
    "                    grid[grid_x, grid_y, 0] = 2\n",
    "                    grid[grid_x, grid_y, 1] = rel_vx\n",
    "                    grid[grid_x, grid_y, 2] = rel_vy\n",
    "\n",
    "        return np.array(grid).transpose(2,0,1)\n",
    "converter = KinematicToGridWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a695efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:23.429333Z",
     "iopub.status.busy": "2024-12-28T13:48:23.429102Z",
     "iopub.status.idle": "2024-12-28T13:48:23.435739Z",
     "shell.execute_reply": "2024-12-28T13:48:23.435009Z"
    },
    "papermill": {
     "duration": 0.017982,
     "end_time": "2024-12-28T13:48:23.436965",
     "exception": false,
     "start_time": "2024-12-28T13:48:23.418983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def init_stack_of_planes(env, history_length=5):\n",
    "    \"\"\"\n",
    "    Initialize a stack of planes representing the grid and velocity information.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance, expected to have the method `unwrapped.observation_type.observe()`\n",
    "             to return the observation.\n",
    "        history_length (int): Number of past frames to stack.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (history_length + 2, grid_height, grid_width) representing the stack of planes.\n",
    "    \"\"\"\n",
    "    # Initialize the KinematicToGridWrapper\n",
    "    converter = KinematicToGridWrapper()\n",
    "\n",
    "    # Collect the initial observations and process each to form planes\n",
    "    grid_planes_list = []\n",
    "    for _ in range(history_length):\n",
    "        obs = env.unwrapped.observation_type.observe()\n",
    "        grid_planes = converter.process_observation(obs, -2, 4 * (4 - 1) + 2)[0]\n",
    "        grid_planes_list.append(grid_planes)\n",
    "\n",
    "    # Convert list to numpy array with shape (history_length, grid_height, grid_width)\n",
    "    stack_of_planes = np.stack(grid_planes_list, axis=0)\n",
    "\n",
    "    # Add two additional planes: obs[1] and obs[2] from the last observation\n",
    "    last_obs = converter.process_observation(env.unwrapped.observation_type.observe(), -2, 4 * (4 - 1) + 2)\n",
    "    plane_6 = last_obs[1]  # Second plane of the latest observation\n",
    "    plane_7 = last_obs[2]  # Third plane of the latest observation\n",
    "\n",
    "    # Concatenate the new planes to the stack\n",
    "    stack_of_planes = np.concatenate(\n",
    "        [stack_of_planes, np.expand_dims(plane_6, axis=0), np.expand_dims(plane_7, axis=0)], axis=0\n",
    "    )\n",
    "\n",
    "    return stack_of_planes\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def get_stack_of_planes(env, old_state, history_length=5):\n",
    "    \"\"\"\n",
    "    Update the stack of planes based on the latest observation.\n",
    "\n",
    "    Args:\n",
    "        env: The environment instance, expected to have the method `unwrapped.observation_type.observe()`\n",
    "             to return the observation.\n",
    "        old_state: A numpy array of shape (history_length + 2, grid_height, grid_width) representing the previous state.\n",
    "        history_length (int): Number of past frames to stack.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (history_length + 2, grid_height, grid_width) representing the updated stack of planes.\n",
    "    \"\"\"\n",
    "    # Initialize the KinematicToGridWrapper\n",
    "    converter = KinematicToGridWrapper()\n",
    "\n",
    "    # Get new observation and process it to form multiple planes\n",
    "    new_obs = converter.process_observation(env.unwrapped.observation_type.observe(), -2, 4 * (4 - 1) + 2)\n",
    "    new_obs_plane_0 = new_obs[0]  # First plane\n",
    "    new_obs_plane_1 = new_obs[1]  # Second plane\n",
    "    new_obs_plane_2 = new_obs[2]  # Third plane\n",
    "\n",
    "    # Shift old_state to remove the oldest frame and append the new one\n",
    "    stack_of_planes = np.roll(old_state, shift=-1, axis=0)\n",
    "    stack_of_planes[-3] = new_obs_plane_0  # Update the third-last plane\n",
    "    stack_of_planes[-2] = new_obs_plane_1  # Update the second-last plane\n",
    "    stack_of_planes[-1] = new_obs_plane_2  # Update the last plane\n",
    "\n",
    "    return stack_of_planes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9a20ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:23.456732Z",
     "iopub.status.busy": "2024-12-28T13:48:23.456492Z",
     "iopub.status.idle": "2024-12-28T13:48:23.533891Z",
     "shell.execute_reply": "2024-12-28T13:48:23.533194Z"
    },
    "papermill": {
     "duration": 0.089039,
     "end_time": "2024-12-28T13:48:23.535334",
     "exception": false,
     "start_time": "2024-12-28T13:48:23.446295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "duration = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26031204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:23.555433Z",
     "iopub.status.busy": "2024-12-28T13:48:23.555176Z",
     "iopub.status.idle": "2024-12-28T13:48:23.559750Z",
     "shell.execute_reply": "2024-12-28T13:48:23.559043Z"
    },
    "papermill": {
     "duration": 0.015767,
     "end_time": "2024-12-28T13:48:23.560988",
     "exception": false,
     "start_time": "2024-12-28T13:48:23.545221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_policy(policy, available_actions):\n",
    "    \"\"\"\n",
    "    Áp dụng softmax cho các xác suất trong policy dựa trên available_actions.\n",
    "\n",
    "    :param policy: Dictionary chứa 5 hành động [0, 1, 2, 3, 4] với các xác suất tương ứng.\n",
    "    :param available_actions: Danh sách các hành động có thể thực hiện (subset của [0, 1, 2, 3, 4]).\n",
    "    :return: Dictionary chứa xác suất mới cho từng hành động (softmax áp dụng với các hành động khả dụng).\n",
    "    \"\"\"\n",
    "    # Lấy các giá trị xác suất tương ứng với available_actions\n",
    "    available_probs = np.array([policy[action] for action in available_actions])\n",
    "\n",
    "    # Áp dụng softmax chỉ trên available_probs\n",
    "    softmax_probs = available_probs / np.sum(available_probs)\n",
    "\n",
    "    # Cập nhật xác suất mới\n",
    "    updated_policy = {action: 0.0 for action in policy}  # Khởi tạo tất cả xác suất bằng 0\n",
    "    for action, prob in zip(available_actions, softmax_probs):\n",
    "        updated_policy[action] = prob\n",
    "\n",
    "    return updated_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856efe4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:23.581070Z",
     "iopub.status.busy": "2024-12-28T13:48:23.580870Z",
     "iopub.status.idle": "2024-12-28T13:48:25.204171Z",
     "shell.execute_reply": "2024-12-28T13:48:25.203224Z"
    },
    "papermill": {
     "duration": 1.635259,
     "end_time": "2024-12-28T13:48:25.206045",
     "exception": false,
     "start_time": "2024-12-28T13:48:23.570786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[133.50334 ,   0.      ,  25.      ,   0.      ,   0.      ],\n",
       "        [142.20247 ,  12.      ,  21.871344,   0.      ,   0.      ],\n",
       "        [150.43214 ,   8.      ,  21.289858,   0.      ,   0.      ],\n",
       "        [159.48709 ,   8.      ,  22.644758,   0.      ,   0.      ],\n",
       "        [167.97862 ,   8.      ,  23.258127,   0.      ,   0.      ],\n",
       "        [176.86223 ,   0.      ,  22.229954,   0.      ,   0.      ],\n",
       "        [185.6481  ,   8.      ,  22.557404,   0.      ,   0.      ],\n",
       "        [194.88641 ,   4.      ,  22.82654 ,   0.      ,   0.      ]],\n",
       "       dtype=float32),\n",
       " {'speed': 25,\n",
       "  'crashed': False,\n",
       "  'action': 2,\n",
       "  'rewards': {'collision_reward': 0.0,\n",
       "   'right_lane_reward': 0.0,\n",
       "   'high_speed_reward': 0.5,\n",
       "   'on_road_reward': 1.0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "config = {\n",
    "    \"observation\": {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 8,\n",
    "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\"],\n",
    "        \"absolute\": True,\n",
    "        \"normalize\": False,\n",
    "        \"order\": \"sorted\",\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"DiscreteMetaAction\",\n",
    "        \"target_speeds\": np.linspace(10, 30, 5)\n",
    "    },\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_density\": 1.6+np.random.rand(),\n",
    "}\n",
    "env = gym.make(\"highway-fast-v0\", config=config, render_mode='rgb_array')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0132dc4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.228472Z",
     "iopub.status.busy": "2024-12-28T13:48:25.228095Z",
     "iopub.status.idle": "2024-12-28T13:48:25.238696Z",
     "shell.execute_reply": "2024-12-28T13:48:25.237818Z"
    },
    "papermill": {
     "duration": 0.022576,
     "end_time": "2024-12-28T13:48:25.240008",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.217432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, env, parent, parent_action, prior_prob, c_puct=2.5):\n",
    "        self.env = copy.deepcopy(env)\n",
    "        self.parent = parent #parent node\n",
    "        self.parent_action = parent_action\n",
    "        self.children = {} #parent.children[action] = child\n",
    "        self._n = 0\n",
    "        self._W = 0\n",
    "        self._P = prior_prob #Xác suất thực hiện hành động parent_action tại parent_node\n",
    "        self.c_puct = c_puct\n",
    "        min_speed = self.env.unwrapped.road.vehicles[0].target_speeds[0]\n",
    "        max_speed = self.env.unwrapped.road.vehicles[0].target_speeds[-1]\n",
    "        self.speed_bonus = (self.env.unwrapped.road.vehicles[0].speed - min_speed)/(max_speed - min_speed)\n",
    "        self.collision = 0\n",
    "        self.brake_penalty = 0\n",
    "        if self.parent_action==4:\n",
    "            self.brake_penalty = 1\n",
    "        if self.env.unwrapped.road.vehicles[0].crashed:\n",
    "            self.collision = 1 + 2*self.speed_bonus\n",
    "        if self.parent is None:\n",
    "            self.stack_of_planes = init_stack_of_planes(self.env)\n",
    "        else:\n",
    "            self.stack_of_planes = get_stack_of_planes(self.env, self.parent.stack_of_planes)\n",
    "    def pucb_score(self):\n",
    "        \"\"\"\n",
    "        Tính PUCB của node\n",
    "        \"\"\"\n",
    "        if self._n == 0:\n",
    "            Q = 0\n",
    "        else:\n",
    "            Q = self._W / self._n\n",
    "\n",
    "        return Q + self.c_puct * self._P * np.sqrt(self.parent._n) / (1 + self._n) + 0.5*self.speed_bonus - 0.4*self.collision - 0.2*self.brake_penalty\n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Chọn node có UCB lớn nhất\n",
    "        \"\"\"\n",
    "        if not self.children:  # Nếu không có node con\n",
    "            return None  # Hoặc raise Exception(\"No children nodes to select from\")\n",
    "        return max(self.children.values(), key=lambda child: child.pucb_score())\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"\n",
    "        Mở rộng cây bằng cách tạo node con\n",
    "        action_priors là một dictionary chứa các xác suất prior của các action\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors.items():\n",
    "            if action not in self.children and prob>0:\n",
    "                #print(f\"expanded {action} with {prob}\")\n",
    "                copy_env = copy.deepcopy(self.env)\n",
    "                copy_env.step(action)\n",
    "                self.children[action] = MCTSNode(copy_env, self, action, prob)\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"\n",
    "        Kiểm tra node có phải là leaf không\n",
    "        \"\"\"\n",
    "        return self.children == {}\n",
    "    def backpropagate(self, result):\n",
    "        \"\"\"\n",
    "        Cập nhật visit count n và tổng điểm W\n",
    "        new Q = new W/ new n\n",
    "        \"\"\"\n",
    "        self._n += 1\n",
    "        self._W += result\n",
    "    def backpropagate_recursive(self, result):\n",
    "        \"\"\"\n",
    "        Cập nhật toàn bộ đường đi từ node hiện tại đến root\n",
    "        \"\"\"\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate_recursive(result)\n",
    "        self.backpropagate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9b7f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.260551Z",
     "iopub.status.busy": "2024-12-28T13:48:25.260302Z",
     "iopub.status.idle": "2024-12-28T13:48:25.268246Z",
     "shell.execute_reply": "2024-12-28T13:48:25.267590Z"
    },
    "papermill": {
     "duration": 0.019547,
     "end_time": "2024-12-28T13:48:25.269383",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.249836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, root, network, c_puct=3.5, n_simulations=10, min_average_speed=23, duration=12):\n",
    "        self.c_puct = c_puct\n",
    "        self.root = root\n",
    "        self._network = network.to(device)\n",
    "        self._n_simulations = n_simulations\n",
    "        self.ego_init_position = root.env.unwrapped.road.vehicles[0].position[0]\n",
    "        self.min_average_speed = min_average_speed\n",
    "        self.duration = duration\n",
    "    def traverse_to_leaf(self):\n",
    "        node = self.root\n",
    "        while not node.is_leaf():\n",
    "            node = node.select()\n",
    "        return node\n",
    "\n",
    "    def rollout(self):\n",
    "        leaf_node = self.traverse_to_leaf()\n",
    "        truncated = leaf_node.env.unwrapped._is_truncated() # True nếu hoàn thành episode\n",
    "        crashed = leaf_node.env.unwrapped.road.vehicles[0].crashed # True nếu ego-vehicle xảy ra va chạm\n",
    "        leaf_state = leaf_node.stack_of_planes\n",
    "        state_tensor = torch.tensor(leaf_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        predicted_policy, predicted_value = self._network(state_tensor)\n",
    "        predicted_policy = {action: prob for action, prob in enumerate(predicted_policy.squeeze().tolist())}\n",
    "        #print(predicted_policy)\n",
    "        available_actions = leaf_node.env.unwrapped.get_available_actions()\n",
    "        #print(available_actions)\n",
    "        updated_policy = softmax_policy(predicted_policy, available_actions)\n",
    "        #print(updated_policy)\n",
    "        predicted_value = predicted_value.item()\n",
    "        if not truncated and not crashed:\n",
    "            leaf_node.expand(updated_policy)\n",
    "        elif truncated:\n",
    "            ego_last_position = leaf_node.env.unwrapped.road.vehicles[0].position[0]\n",
    "            ego_average_speed = (ego_last_position - self.ego_init_position)/(self.duration-1)\n",
    "            confidence_score = ego_average_speed / self.min_average_speed\n",
    "            if confidence_score >=1.0:\n",
    "                predicted_value = 1.0\n",
    "            else:\n",
    "                predicted_value = 0.0\n",
    "        elif crashed:\n",
    "            predicted_value = -1.0\n",
    "        leaf_node.backpropagate_recursive(predicted_value)\n",
    "\n",
    "    def move_to_new_root(self, action):\n",
    "        \"\"\"\n",
    "        Chuyển gốc của cây MCTS tới node con tương ứng với hành động được chọn.\n",
    "        \"\"\"\n",
    "        if action in self.root.children:\n",
    "            self.root = self.root.children[action]  # Di chuyển gốc tới node con\n",
    "            self.root.parent = None  # Ngắt liên kết với node cha để giảm bộ nhớ\n",
    "        else:\n",
    "            # Nếu node con không tồn tại, khởi tạo lại cây tại node gốc mới\n",
    "            raise ValueError(\"Hành động không có trong cây hiện tại.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eddfd3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.289772Z",
     "iopub.status.busy": "2024-12-28T13:48:25.289548Z",
     "iopub.status.idle": "2024-12-28T13:48:25.304977Z",
     "shell.execute_reply": "2024-12-28T13:48:25.304325Z"
    },
    "papermill": {
     "duration": 0.027162,
     "end_time": "2024-12-28T13:48:25.306373",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.279211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "import torch\n",
    "\n",
    "class AlphaZeroTrainer:\n",
    "    def __init__(self, network, env, c_puct=2, n_simulations=10, learning_rate=0.001, batch_size=32, epochs=10):\n",
    "        self.network = network  # AlphaZeroNetwork\n",
    "        self.env = env\n",
    "        self.c_puct = c_puct\n",
    "        self.n_simulations = n_simulations\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.training_data = []  # Lưu trữ dữ liệu huấn luyện dạng (state, policy, value)\n",
    "        self.action_list = []\n",
    "\n",
    "    def self_play(self, seed=21):\n",
    "        \"\"\"\n",
    "        Tạo dữ liệu huấn luyện thông qua self-play với MCTS.\n",
    "        \"\"\"\n",
    "        # Khởi tạo lại môi trường và trạng thái ban đầu\n",
    "        self.env.reset(seed=seed)\n",
    "        state = init_stack_of_planes(env)\n",
    "        done = self.env.unwrapped._is_truncated() or self.env.unwrapped._is_terminated()\n",
    "\n",
    "        # Tạo gốc của cây MCTS\n",
    "        root_node = MCTSNode(self.env, parent=None, parent_action=None, prior_prob=1.0, c_puct=self.c_puct)\n",
    "        mcts = MCTS(root=root_node, network=self.network, c_puct=self.c_puct, n_simulations=self.n_simulations, duration=duration)\n",
    "\n",
    "        while not done:\n",
    "            # Thực hiện MCTS rollout để tính xác suất hành động\n",
    "            state = get_stack_of_planes(env, state)\n",
    "            for _ in range(self.n_simulations):\n",
    "                mcts.rollout()\n",
    "            # Thu thập xác suất hành động và giá trị của trạng thái hiện tại\n",
    "            action_probs = {action: 0.0 for action in range(5)}  # Khởi tạo xác suất của tất cả hành động là 0\n",
    "            for action, child in root_node.children.items():\n",
    "                action_probs[action] = child._n / (root_node._n - 1)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            #print(state_tensor.shape)\n",
    "            predicted_value = root_node._W / root_node._n if root_node._n > 0 else 0\n",
    "\n",
    "            # Lưu trữ dữ liệu huấn luyện\n",
    "\n",
    "\n",
    "            # Chọn hành động dựa trên xác suất từ MCTS\n",
    "            action = max(action_probs, key=action_probs.get)\n",
    "            self.action_list.append(action)\n",
    "            self.env.step(action)\n",
    "            #print(f\"action chosen: {action}\")\n",
    "            self.training_data.append((state_tensor, action_probs, predicted_value, action))\n",
    "            #(env.unwrapped.road.vehicles[0].target_lane_index[2])\n",
    "\n",
    "            # Di chuyển gốc của MCTS đến node con tương ứng với hành động được chọn\n",
    "            if action in root_node.children:\n",
    "                mcts.move_to_new_root(action)\n",
    "                root_node = mcts.root  # Cập nhật root_node cho vòng lặp kế tiếp\n",
    "            else:\n",
    "                raise ValueError(\"Action không tồn tại trong cây MCTS.\")\n",
    "\n",
    "            # Cập nhật trạng thái và kiểm tra điều kiện kết thúc\n",
    "            done = self.env.unwrapped._is_truncated() or self.env.unwrapped._is_terminated()\n",
    "        print(\"end self-play\")\n",
    "\n",
    "    def train(self):\n",
    "        self.network.to(device)\n",
    "        states, policies, values, actions = zip(*self.training_data)\n",
    "        states = torch.cat(states).to(device)\n",
    "        policies = torch.tensor([list(policy.values()) for policy in policies], dtype=torch.float32).to(device)\n",
    "        values = torch.tensor(values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "\n",
    "        # Weighted sampling\n",
    "        class_counts = torch.bincount(actions)\n",
    "        class_weights = 1.0 / (class_counts.float() + 1e-6)\n",
    "        sample_weights = class_weights[actions]\n",
    "\n",
    "        sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "        dataset = TensorDataset(states, policies, values, actions)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, sampler=sampler)\n",
    "\n",
    "        # Khởi tạo các danh sách lưu trữ loss\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.total_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_policy_loss = 0\n",
    "            epoch_value_loss = 0\n",
    "            epoch_total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            for state_batch, policy_batch, value_batch, action_batch in dataloader:\n",
    "                # Move batches to device\n",
    "                state_batch = state_batch.to(device)\n",
    "                policy_batch = policy_batch.to(device)\n",
    "                value_batch = value_batch.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                predicted_policy, predicted_value = self.network(state_batch)\n",
    "\n",
    "                # Losses\n",
    "                kl_loss = F.kl_div(\n",
    "                    F.log_softmax(predicted_policy, dim=-1),\n",
    "                    policy_batch,\n",
    "                    reduction='batchmean'\n",
    "                )\n",
    "                value_loss = F.mse_loss(predicted_value, value_batch)\n",
    "                loss = 0.9*policy_loss + 0.1*value_loss\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Cộng dồn loss\n",
    "                epoch_policy_loss += policy_loss.item()\n",
    "                epoch_value_loss += value_loss.item()\n",
    "                epoch_total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "            # Tính loss trung bình cho mỗi epoch\n",
    "            avg_policy_loss = epoch_policy_loss / batch_count\n",
    "            avg_value_loss = epoch_value_loss / batch_count\n",
    "            avg_total_loss = epoch_total_loss / batch_count\n",
    "\n",
    "            # Lưu loss vào danh sách\n",
    "            self.policy_losses.append(avg_policy_loss)\n",
    "            self.value_losses.append(avg_value_loss)\n",
    "            self.total_losses.append(avg_total_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, value loss: {avg_value_loss}, policy loss: {avg_policy_loss}, Loss: {avg_total_loss}\")\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, path=\"alphazero_model.pth\"):\n",
    "        torch.save(self.network.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path=\"alphazero_model.pth\"):\n",
    "        self.network.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4842ccbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.326983Z",
     "iopub.status.busy": "2024-12-28T13:48:25.326787Z",
     "iopub.status.idle": "2024-12-28T13:48:25.331054Z",
     "shell.execute_reply": "2024-12-28T13:48:25.330407Z"
    },
    "papermill": {
     "duration": 0.015698,
     "end_time": "2024-12-28T13:48:25.332108",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.316410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def env_init(duration):\n",
    "    config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"Kinematics\",\n",
    "            \"vehicles_count\": 8,\n",
    "            \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"heading\"],\n",
    "            \"absolute\": True,\n",
    "            \"normalize\": False,\n",
    "            \"order\": \"sorted\",\n",
    "        },\n",
    "        \"action\": {\n",
    "            \"type\": \"DiscreteMetaAction\",\n",
    "            \"target_speeds\": np.linspace(10, 30, 5)\n",
    "        },\n",
    "        \"lanes_count\": 4,\n",
    "        \"vehicles_density\": 1.6+np.random.rand(),\n",
    "        \"duration\": duration,\n",
    "    }\n",
    "    env = gym.make(\"highway-fast-v0\", config=config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f354d271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.353268Z",
     "iopub.status.busy": "2024-12-28T13:48:25.353038Z",
     "iopub.status.idle": "2024-12-28T13:48:25.372630Z",
     "shell.execute_reply": "2024-12-28T13:48:25.372026Z"
    },
    "papermill": {
     "duration": 0.031273,
     "end_time": "2024-12-28T13:48:25.373956",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.342683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = env_init(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f50a4c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.395665Z",
     "iopub.status.busy": "2024-12-28T13:48:25.395428Z",
     "iopub.status.idle": "2024-12-28T13:48:25.589474Z",
     "shell.execute_reply": "2024-12-28T13:48:25.588491Z"
    },
    "papermill": {
     "duration": 0.207509,
     "end_time": "2024-12-28T13:48:25.591240",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.383731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "network = ResNetRNNPolicyValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ef09be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.612026Z",
     "iopub.status.busy": "2024-12-28T13:48:25.611777Z",
     "iopub.status.idle": "2024-12-28T13:48:25.615729Z",
     "shell.execute_reply": "2024-12-28T13:48:25.614982Z"
    },
    "papermill": {
     "duration": 0.01569,
     "end_time": "2024-12-28T13:48:25.617200",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.601510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = AlphaZeroTrainer(network, env, c_puct=2.0, n_simulations=150, learning_rate=0.001, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c073655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.637616Z",
     "iopub.status.busy": "2024-12-28T13:48:25.637403Z",
     "iopub.status.idle": "2024-12-28T13:48:25.642230Z",
     "shell.execute_reply": "2024-12-28T13:48:25.641376Z"
    },
    "papermill": {
     "duration": 0.016809,
     "end_time": "2024-12-28T13:48:25.643627",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.626818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def training_pipeline(trainer, iterations=3, init_duration=20, duration_increase=5):\n",
    "    trainer.network.train()\n",
    "    for i in range(iterations):\n",
    "        trainer.training_data = []\n",
    "        env = env_init(duration=init_duration + duration_increase*i)\n",
    "        trainer.env = env\n",
    "        print(f\"iter: {i}\")\n",
    "        j = 0\n",
    "        while len(trainer.training_data)<2500:\n",
    "            print(f\"self-play: {j}\")\n",
    "            trainer.self_play(seed=j)\n",
    "            j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b0653d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T13:48:25.664744Z",
     "iopub.status.busy": "2024-12-28T13:48:25.664482Z",
     "iopub.status.idle": "2024-12-29T00:42:39.426474Z",
     "shell.execute_reply": "2024-12-29T00:42:39.425571Z"
    },
    "papermill": {
     "duration": 39253.790598,
     "end_time": "2024-12-29T00:42:39.444377",
     "exception": false,
     "start_time": "2024-12-28T13:48:25.653779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "self-play: 0\n",
      "end self-play\n",
      "self-play: 1\n",
      "end self-play\n",
      "self-play: 2\n",
      "end self-play\n",
      "self-play: 3\n",
      "end self-play\n",
      "self-play: 4\n",
      "end self-play\n",
      "self-play: 5\n",
      "end self-play\n",
      "self-play: 6\n",
      "end self-play\n",
      "self-play: 7\n",
      "end self-play\n",
      "self-play: 8\n",
      "end self-play\n",
      "self-play: 9\n",
      "end self-play\n",
      "self-play: 10\n",
      "end self-play\n",
      "self-play: 11\n",
      "end self-play\n",
      "self-play: 12\n",
      "end self-play\n",
      "self-play: 13\n",
      "end self-play\n",
      "self-play: 14\n",
      "end self-play\n",
      "self-play: 15\n",
      "end self-play\n",
      "self-play: 16\n",
      "end self-play\n",
      "self-play: 17\n",
      "end self-play\n",
      "self-play: 18\n",
      "end self-play\n",
      "self-play: 19\n",
      "end self-play\n",
      "self-play: 20\n",
      "end self-play\n",
      "self-play: 21\n",
      "end self-play\n",
      "self-play: 22\n",
      "end self-play\n",
      "self-play: 23\n",
      "end self-play\n",
      "self-play: 24\n",
      "end self-play\n",
      "self-play: 25\n",
      "end self-play\n",
      "self-play: 26\n",
      "end self-play\n",
      "self-play: 27\n",
      "end self-play\n",
      "self-play: 28\n",
      "end self-play\n",
      "self-play: 29\n",
      "end self-play\n",
      "self-play: 30\n",
      "end self-play\n",
      "self-play: 31\n",
      "end self-play\n",
      "self-play: 32\n",
      "end self-play\n",
      "self-play: 33\n",
      "end self-play\n",
      "self-play: 34\n",
      "end self-play\n",
      "self-play: 35\n",
      "end self-play\n",
      "self-play: 36\n",
      "end self-play\n",
      "self-play: 37\n",
      "end self-play\n",
      "self-play: 38\n",
      "end self-play\n",
      "self-play: 39\n",
      "end self-play\n",
      "self-play: 40\n",
      "end self-play\n",
      "self-play: 41\n",
      "end self-play\n",
      "self-play: 42\n",
      "end self-play\n",
      "self-play: 43\n",
      "end self-play\n",
      "self-play: 44\n",
      "end self-play\n",
      "self-play: 45\n",
      "end self-play\n",
      "self-play: 46\n",
      "end self-play\n",
      "self-play: 47\n",
      "end self-play\n",
      "self-play: 48\n",
      "end self-play\n",
      "self-play: 49\n",
      "end self-play\n",
      "self-play: 50\n",
      "end self-play\n",
      "self-play: 51\n",
      "end self-play\n",
      "self-play: 52\n",
      "end self-play\n",
      "self-play: 53\n",
      "end self-play\n",
      "self-play: 54\n",
      "end self-play\n",
      "self-play: 55\n",
      "end self-play\n",
      "self-play: 56\n",
      "end self-play\n",
      "self-play: 57\n",
      "end self-play\n",
      "self-play: 58\n",
      "end self-play\n",
      "self-play: 59\n",
      "end self-play\n",
      "self-play: 60\n",
      "end self-play\n",
      "self-play: 61\n",
      "end self-play\n",
      "self-play: 62\n",
      "end self-play\n",
      "self-play: 63\n",
      "end self-play\n",
      "self-play: 64\n",
      "end self-play\n",
      "self-play: 65\n",
      "end self-play\n",
      "self-play: 66\n",
      "end self-play\n",
      "self-play: 67\n",
      "end self-play\n",
      "self-play: 68\n",
      "end self-play\n",
      "self-play: 69\n",
      "end self-play\n",
      "self-play: 70\n",
      "end self-play\n",
      "self-play: 71\n",
      "end self-play\n",
      "self-play: 72\n",
      "end self-play\n",
      "self-play: 73\n",
      "end self-play\n",
      "self-play: 74\n",
      "end self-play\n",
      "self-play: 75\n",
      "end self-play\n",
      "self-play: 76\n",
      "end self-play\n",
      "self-play: 77\n",
      "end self-play\n",
      "self-play: 78\n",
      "end self-play\n",
      "self-play: 79\n",
      "end self-play\n",
      "self-play: 80\n",
      "end self-play\n",
      "self-play: 81\n",
      "end self-play\n",
      "self-play: 82\n",
      "end self-play\n",
      "self-play: 83\n",
      "end self-play\n",
      "self-play: 84\n",
      "end self-play\n",
      "self-play: 85\n",
      "end self-play\n",
      "self-play: 86\n",
      "end self-play\n",
      "self-play: 87\n",
      "end self-play\n",
      "self-play: 88\n",
      "end self-play\n",
      "self-play: 89\n",
      "end self-play\n",
      "self-play: 90\n",
      "end self-play\n",
      "self-play: 91\n",
      "end self-play\n",
      "self-play: 92\n",
      "end self-play\n",
      "self-play: 93\n",
      "end self-play\n",
      "self-play: 94\n",
      "end self-play\n",
      "self-play: 95\n",
      "end self-play\n",
      "self-play: 96\n",
      "end self-play\n",
      "self-play: 97\n",
      "end self-play\n",
      "self-play: 98\n",
      "end self-play\n",
      "self-play: 99\n",
      "end self-play\n",
      "self-play: 100\n",
      "end self-play\n",
      "self-play: 101\n",
      "end self-play\n",
      "self-play: 102\n",
      "end self-play\n",
      "self-play: 103\n",
      "end self-play\n",
      "self-play: 104\n",
      "end self-play\n",
      "self-play: 105\n",
      "end self-play\n",
      "self-play: 106\n",
      "end self-play\n",
      "self-play: 107\n",
      "end self-play\n",
      "self-play: 108\n",
      "end self-play\n",
      "self-play: 109\n",
      "end self-play\n",
      "self-play: 110\n",
      "end self-play\n",
      "self-play: 111\n",
      "end self-play\n",
      "self-play: 112\n",
      "end self-play\n",
      "self-play: 113\n",
      "end self-play\n",
      "self-play: 114\n",
      "end self-play\n",
      "self-play: 115\n",
      "end self-play\n",
      "self-play: 116\n",
      "end self-play\n",
      "self-play: 117\n",
      "end self-play\n",
      "self-play: 118\n",
      "end self-play\n",
      "self-play: 119\n",
      "end self-play\n",
      "self-play: 120\n",
      "end self-play\n",
      "self-play: 121\n",
      "end self-play\n",
      "self-play: 122\n",
      "end self-play\n",
      "self-play: 123\n",
      "end self-play\n",
      "self-play: 124\n",
      "end self-play\n",
      "self-play: 125\n",
      "end self-play\n"
     ]
    }
   ],
   "source": [
    "training_pipeline(trainer, iterations=1, init_duration=20, duration_increase=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59a3de0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T00:42:39.561772Z",
     "iopub.status.busy": "2024-12-29T00:42:39.561459Z",
     "iopub.status.idle": "2024-12-29T00:42:39.565117Z",
     "shell.execute_reply": "2024-12-29T00:42:39.564456Z"
    },
    "papermill": {
     "duration": 0.021977,
     "end_time": "2024-12-29T00:42:39.566419",
     "exception": false,
     "start_time": "2024-12-29T00:42:39.544442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = trainer.training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "216849f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T00:42:39.599417Z",
     "iopub.status.busy": "2024-12-29T00:42:39.599109Z",
     "iopub.status.idle": "2024-12-29T00:42:39.980310Z",
     "shell.execute_reply": "2024-12-29T00:42:39.979546Z"
    },
    "papermill": {
     "duration": 0.399083,
     "end_time": "2024-12-29T00:42:39.982042",
     "exception": false,
     "start_time": "2024-12-29T00:42:39.582959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"training_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(training_data, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39335.135682,
   "end_time": "2024-12-29T00:42:42.264057",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-28T13:47:07.128375",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
